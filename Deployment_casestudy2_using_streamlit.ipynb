{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deployment_casestudy2_using_streamlit.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aR56Nn-qq56",
        "outputId": "56f8a689-7673-4231-ca49-f799d0998cb8"
      },
      "source": [
        "!pip -q install streamlit\n",
        "!pip -q install pyngrok\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 8.3 MB 26.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 56.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 75 kB 5.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 111 kB 75.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 60.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 124 kB 73.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 788 kB 56.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 370 kB 59.4 MB/s \n",
            "\u001b[?25h  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.20 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.4.1 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.28.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 745 kB 36.8 MB/s \n",
            "\u001b[?25h  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4QhRSfG4dUd",
        "outputId": "2e22bcb3-0634-4f65-8a79-83ca2dcf3d37"
      },
      "source": [
        "!pip install opencv-python\n",
        "!pip install easyocr"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.19.5)\n",
            "Collecting easyocr\n",
            "  Downloading easyocr-1.4.1-py3-none-any.whl (63.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 63.6 MB 35 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from easyocr) (1.4.1)\n",
            "Requirement already satisfied: Pillow<8.3.0 in /usr/local/lib/python3.7/dist-packages (from easyocr) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from easyocr) (1.19.5)\n",
            "Collecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (37.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 37.1 MB 91 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from easyocr) (1.9.0+cu102)\n",
            "Collecting python-bidi\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from easyocr) (3.13)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from easyocr) (0.16.2)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.7/dist-packages (from easyocr) (0.10.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->easyocr) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from python-bidi->easyocr) (1.15.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->easyocr) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->easyocr) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->easyocr) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->easyocr) (2.6.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (2.8.2)\n",
            "Installing collected packages: python-bidi, opencv-python-headless, easyocr\n",
            "Successfully installed easyocr-1.4.1 opencv-python-headless-4.5.3.56 python-bidi-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4EtL3N8ymKj"
      },
      "source": [
        "from pyngrok import ngrok"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vanSM5kCxzbG",
        "outputId": "fc52ee90-b8d5-41c7-b581-8186fbc6800b"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import easyocr\n",
        "import requests\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import urllib.request\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "\n",
        "#st.title('Predicting a Meme Dank or Not?')\n",
        "html_temp = \"\"\"\n",
        "    <div style=\"background-color:grey;padding:10px\">\n",
        "    <h2 style=\"color:black;text-align:center;\">Predicting a Meme Dank or Not? </h2>\n",
        "    </div>\"\"\"\n",
        "st.markdown(html_temp,unsafe_allow_html=True)\n",
        "\n",
        "predict_meme = st.file_uploader(\"Select a File\")\n",
        "reader = easyocr.Reader(['en'],gpu = True)\n",
        "tokenizer = pd.read_pickle('/content/tokenizer.pkl')\n",
        "model = load_model('/content/drive/MyDrive/case_study_2/output/my_model.h5')\n",
        "\n",
        "# Tokenize and lemmatize\n",
        "def lemmatize_stemming(text):\n",
        "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    output=[]\n",
        "    for token in gensim.utils.simple_preprocess(text) :\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            output.append(lemmatize_stemming(token))\n",
        "            \n",
        "    return output\n",
        "\n",
        "if predict_meme is not None:\n",
        "    query_meme = pd.read_csv(predict_meme)\n",
        "    content = requests.get(query_meme['media'].values[0]).content\n",
        "    image = Image.open(BytesIO(content))\n",
        "    st.image(image,use_column_width=True)\n",
        "    urllib.request.urlretrieve(query_meme.media.values[0],'image.jpg')\n",
        "    output = reader.readtext('image.jpg',decoder = 'greedy', beamWidth= 5, batch_size = 1,\n",
        "                            workers = 0, allowlist = None, blocklist = None, detail = 1,\n",
        "                            rotation_info = None, paragraph = False, min_size = 20,\n",
        "                            contrast_ths = 0.1,adjust_contrast = 0.5, filter_ths = 0.003,\n",
        "                            text_threshold = 0.7, low_text = 0.4, link_threshold = 0.4,\n",
        "                            canvas_size = 2560, mag_ratio = 1.,\n",
        "                            slope_ths = 0.1 , ycenter_ths = 0.5, height_ths = 0.5,\n",
        "                            width_ths = 0.5, y_ths = 0.5, x_ths = 1.0, add_margin = 0.1, output_format='standard')\n",
        "    text = \"\"  \n",
        "    for i in range(len(output)):\n",
        "            text = text + \" \" +''.join(output[i][1])   \n",
        "            text = text.strip()\n",
        "    text = query_meme['title'].values[0] + ' ' + text\n",
        "    processed_words = preprocess(text)\n",
        "    processed_words = [word for word in processed_words if word not in stop] \n",
        "    memetext = ' '.join(processed_words)\n",
        "    query_meme['Text'] = memetext\n",
        "    sequences = tokenizer.texts_to_sequences(query_meme['Text'])\n",
        "    sequences_matrix = pad_sequences(sequences,maxlen=225)\n",
        "    response = requests.get(query_meme['media'].values[0],stream=True)\n",
        "            #opening the image\n",
        "    image= np.array(Image.open(response.raw))\n",
        "    image= np.resize(image,(299,299,3))\n",
        "    image = image.astype('float32')\n",
        "    image /= 255\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    if st.button(\"Predict\"):\n",
        "        prediction = model.predict([sequences_matrix[:1],np.array(image[:1],np.float32)])\n",
        "        st.write(\"Meme is :\")\n",
        "        if prediction<0.5:\n",
        "            st.success('NOT DANK')\n",
        "        else:\n",
        "            st.success('Dank')\n",
        "    "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ymfzuY03pyV",
        "outputId": "674c592c-41dc-423b-d3e8-f0374f7ca0b3"
      },
      "source": [
        "!ngrok authtoken 1vaAzvxuZoTRjkNKSpUk282c9n1_2YZXiZuLEh4TAGMNgZBFN"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1ldVJZ847q6",
        "outputId": "2358e218-d35a-431a-9e1b-8fff7dca1222"
      },
      "source": [
        "public_url = ngrok.connect(port='80')\n",
        "print(public_url)\n",
        "!streamlit run --server.port 80 app.py >/dev/null"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NgrokTunnel: \"http://a3c3-34-86-44-114.ngrok.io\" -> \"http://localhost:80\"\n",
            "2021-09-30 09:47:36.852 'pattern' package not found; tag filters are not available for English\n",
            "2021-09-30 09:47:44.470067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 09:47:44.477488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 09:47:44.478142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 09:47:44.479144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 09:47:44.479762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 09:47:44.480317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 09:47:44.882768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 09:47:44.884279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 09:47:44.885549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 09:47:44.886308: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-09-30 09:47:44.886383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12737 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "2021-09-30 09:50:16.704427: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "2021-09-30 09:50:17.534233: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
          ]
        }
      ]
    }
  ]
}